{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Notes on Types and Programming Languages \u00b6 These notes mostly follow the Types and Programming Languages 1 [TaPL] book by Bejamin C. Pierce, but may also contain additional remarks and may link to other external resources, including but not limited to: blog posts, conference talks, journal articles, papers, code, repositories, etc. When no inline citation is specified, assume the material is adapted from [TaPL]. Get the Types and Programming Languages book Language references These notes also have a Rust 2 language lean, but we also use a wide variety of programming language snippets to demonstrate various type systems and features, as well as their possible caveats and limitations. These programming languages include but are not limited to: Rust C C++ Haskell Idris JavaScript TypeScript Java Scala Go Citation conventions We use the IEEE Access citation style for References . We use one of the following inline-citation style: Author-year Key Examples: [John1999] [John2000a] Abbreviation Key Examples: [RFC793] [CLRS] as the key to the References . https://www.cis.upenn.edu/~bcpierce/tapl/ \u21a9 https://github.com/rust-lang/rust . \u21a9","title":"Notes on Types and Programming Languages"},{"location":"#notes-on-types-and-programming-languages","text":"These notes mostly follow the Types and Programming Languages 1 [TaPL] book by Bejamin C. Pierce, but may also contain additional remarks and may link to other external resources, including but not limited to: blog posts, conference talks, journal articles, papers, code, repositories, etc. When no inline citation is specified, assume the material is adapted from [TaPL]. Get the Types and Programming Languages book Language references These notes also have a Rust 2 language lean, but we also use a wide variety of programming language snippets to demonstrate various type systems and features, as well as their possible caveats and limitations. These programming languages include but are not limited to: Rust C C++ Haskell Idris JavaScript TypeScript Java Scala Go Citation conventions We use the IEEE Access citation style for References . We use one of the following inline-citation style: Author-year Key Examples: [John1999] [John2000a] Abbreviation Key Examples: [RFC793] [CLRS] as the key to the References . https://www.cis.upenn.edu/~bcpierce/tapl/ \u21a9 https://github.com/rust-lang/rust . \u21a9","title":"Notes on Types and Programming Languages"},{"location":"references/","text":"References \u00b6 work in progerss [TaPL] B. C. Pierce, Types and Programming Languages . Cambridge, MA: The MIT Press, 2002. [Jung2018] R. Jung, J.-H. Jourdan, R. Krebbers, and D. Dreyer, \u201cRustBelt: Securing the Foundations of the Rust Programming Language,\u201d Proceedings of the ACM on Programming Languages , vol. 2, no. POPL, pp. 1\u201334, 2018.","title":"References"},{"location":"references/#references","text":"work in progerss [TaPL] B. C. Pierce, Types and Programming Languages . Cambridge, MA: The MIT Press, 2002. [Jung2018] R. Jung, J.-H. Jourdan, R. Krebbers, and D. Dreyer, \u201cRustBelt: Securing the Foundations of the Rust Programming Language,\u201d Proceedings of the ACM on Programming Languages , vol. 2, no. POPL, pp. 1\u201334, 2018.","title":"References"},{"location":"introduction/brief-history/","text":"Brief History \u00b6 This list is very incomplete, and very outdated. But these are some of the very foundational papers of the modern-day type system studies. This table is taken from TaPL with modifications to add a few more important papers in more recent periods. Time Period Topic Author/Date 1870s Formal logic Frege (1879) 1900s Formalization of mathematics Whitehead and Russel (1910) 1930s Untyped lambda calculus Church (1941) 1940s Simply-typed Lambda-Calculus Church(1940); Curry and Feys (1958) 1950s Fortran Backus (1981) Algol-60 Naur et al. (1963) 1960s Automath de Bruijin (1980) Simula Birtwistle et al. (1970) Curry-Howard correspondence Howard (1980) Algol-68 van Wijingaarden et al. (1975) 1970s Pascal Wirth (1971) Martin-L\u00f6f type theory Martin-L\u00f6f (1973, 1982) System F, \\(F^\\omega\\) Girard (1972) Polymorphic Lambda-Calculus Reynolds (1974) CLU Liskov et al. (1981) Polymorphic Type Inference Milner (1978), Damas and Milner (1982) ML Gordon, Milner and Wadsworth (1979) Intersection Types Coppo and Dezani (1978); Coppo, Dezani and Sall\u00e9 (1979); Pottinger (1980) 1980s NuPRL Constable et al. (1986) Temporal logic verification Clarke, Emerson, Sistla (1986) Subtyping Reynolds (1980); Cardelli (1984); Mitchell (1984) Coq Coquand, Huet (1991) ADTs as Existential Types Mitchell and Plotkin (1988) Calculus of Constructions Coquand (1985); Coquand and Huet (1988) Linear Logic Girard (1987); Girard et al. (1989) Bounded Quantification Cardelli and Wegner (1985); Curien and Ghelli (1992); Cardelli et al. (1994) Edinburgh logical framework Harper, Honsell and Plotkin (1992) Forsythe Reynolds (1988) Pure type systems Terlouw (1989); Berardi (1988); Barendregt (1991) Dependent types and modularity Burstall and Lampson (1984); MacQueen (1986) Quest Cardelli (1991); Effect systems Gifford et al. (1987); Talpin and Jouvelot (1992) Row variables and extensible records Wand (1987); Remy (1989); Cardelli and Mitchell (1991) 1990s Higher-order subtyping Cardelli (1990); Cardellli and Longo (1991) Coq Paulin (1991) Temporal logic for reactive/concurrent systems Manna and Pnueli (1992) Refinement types for ML Freeman (1991) Temporal logic Alur (1994) Typed intermediate languages Tarditi, Morrisett, et al. (1996) Object calculus Abadi and Cardelli (1996) Translucent types and modularity Harper and Lillibridge (1996); Leroy (1994) Typed assembly lanugage Morrisett et al. (1998) 2000s Separation logic Reynolds (2002) Generalized algebraic data types Cheney and Hinze (2003); Xi, Chen and Chen (2003); Sheard and Pasalic (2004) Higher-order modules Dreyer (2003) Traits Sch\u00e4rli et al. (2003) Arbitrary rank types Jones (2005) Agda Norell (2007) Gradual typing Siek and Taha (2006) Concurrent separation logic Brookes (2007) Typestate-orientated programming Aldrich (2009) 2010s Refinement types for Haskell Vazou (2014) Homotopy type theory Univalent Foundations (2013) Semantic typing Ralf Jung et al. (2018) Idris2 Brady (2018) Cartesian cubical type theory Angiuli (2018)","title":"Brief History"},{"location":"introduction/brief-history/#brief-history","text":"This list is very incomplete, and very outdated. But these are some of the very foundational papers of the modern-day type system studies. This table is taken from TaPL with modifications to add a few more important papers in more recent periods. Time Period Topic Author/Date 1870s Formal logic Frege (1879) 1900s Formalization of mathematics Whitehead and Russel (1910) 1930s Untyped lambda calculus Church (1941) 1940s Simply-typed Lambda-Calculus Church(1940); Curry and Feys (1958) 1950s Fortran Backus (1981) Algol-60 Naur et al. (1963) 1960s Automath de Bruijin (1980) Simula Birtwistle et al. (1970) Curry-Howard correspondence Howard (1980) Algol-68 van Wijingaarden et al. (1975) 1970s Pascal Wirth (1971) Martin-L\u00f6f type theory Martin-L\u00f6f (1973, 1982) System F, \\(F^\\omega\\) Girard (1972) Polymorphic Lambda-Calculus Reynolds (1974) CLU Liskov et al. (1981) Polymorphic Type Inference Milner (1978), Damas and Milner (1982) ML Gordon, Milner and Wadsworth (1979) Intersection Types Coppo and Dezani (1978); Coppo, Dezani and Sall\u00e9 (1979); Pottinger (1980) 1980s NuPRL Constable et al. (1986) Temporal logic verification Clarke, Emerson, Sistla (1986) Subtyping Reynolds (1980); Cardelli (1984); Mitchell (1984) Coq Coquand, Huet (1991) ADTs as Existential Types Mitchell and Plotkin (1988) Calculus of Constructions Coquand (1985); Coquand and Huet (1988) Linear Logic Girard (1987); Girard et al. (1989) Bounded Quantification Cardelli and Wegner (1985); Curien and Ghelli (1992); Cardelli et al. (1994) Edinburgh logical framework Harper, Honsell and Plotkin (1992) Forsythe Reynolds (1988) Pure type systems Terlouw (1989); Berardi (1988); Barendregt (1991) Dependent types and modularity Burstall and Lampson (1984); MacQueen (1986) Quest Cardelli (1991); Effect systems Gifford et al. (1987); Talpin and Jouvelot (1992) Row variables and extensible records Wand (1987); Remy (1989); Cardelli and Mitchell (1991) 1990s Higher-order subtyping Cardelli (1990); Cardellli and Longo (1991) Coq Paulin (1991) Temporal logic for reactive/concurrent systems Manna and Pnueli (1992) Refinement types for ML Freeman (1991) Temporal logic Alur (1994) Typed intermediate languages Tarditi, Morrisett, et al. (1996) Object calculus Abadi and Cardelli (1996) Translucent types and modularity Harper and Lillibridge (1996); Leroy (1994) Typed assembly lanugage Morrisett et al. (1998) 2000s Separation logic Reynolds (2002) Generalized algebraic data types Cheney and Hinze (2003); Xi, Chen and Chen (2003); Sheard and Pasalic (2004) Higher-order modules Dreyer (2003) Traits Sch\u00e4rli et al. (2003) Arbitrary rank types Jones (2005) Agda Norell (2007) Gradual typing Siek and Taha (2006) Concurrent separation logic Brookes (2007) Typestate-orientated programming Aldrich (2009) 2010s Refinement types for Haskell Vazou (2014) Homotopy type theory Univalent Foundations (2013) Semantic typing Ralf Jung et al. (2018) Idris2 Brady (2018) Cartesian cubical type theory Angiuli (2018)","title":"Brief History"},{"location":"introduction/preface/","text":"Preface \u00b6 Welcome to Notes on Types and Programming Languages. Obligatory start by a relevant XKCD : XKCD 1537: Types","title":"Preface"},{"location":"introduction/preface/#preface","text":"Welcome to Notes on Types and Programming Languages. Obligatory start by a relevant XKCD : XKCD 1537: Types","title":"Preface"},{"location":"introduction/type-systems-good-for/","text":"What are Type Systems Good For? \u00b6 Error Detection \u00b6 Static type checking systems can detect and thus catch certain errors early, at compile-time. How much of such errors that can be caught by the type checking system depends on the type system's expressiveness . A more expressive type system typically allows the programmer to encode more information about data types and operations defined w.r.t. to those data types, which the type system can use these encoded information to check for certain errors. Languages which support dependent typing can express array bounds checks by encoding an array or vector's length in the types, so that bounds checking can be eliminated at run-time because the static type system can prove the absence of certain out-of-bounds access at compile-time. An example in Idris : -- Natural numbers: base case 0, or inductively define as successors of 0. -- e.g. 1 == S Z, 2 == S (S Z), and so on for all natural numbers. data Nat = Z | S Nat -- A vector is also inductively defined; it is either an empty list `Nil`, or elements appended -- to the empty list via `(::)`. Notice that the vector's length is encoded in its type, -- specifically with it's `Nat` type in `Vector k a`, which is parametric over both its length `k` -- and element type `a`. data Vector : Nat -> Type -> Type where Nil : Vector Z a ( :: ) : a -> Vector k a -> Vector ( S k ) a -- When two vectors are concatenated, the length of the resulting vector `r` is the sum of the -- lengths of the two input vectors `n` and `m` respectively. -- This invariant is encoded at the type level by the type-level expression `(n + m)`. concat : Vector n a -> Vector m a -> Vector ( n + m ) a concat Nil ys = ys concat ( x :: xs ) ys = x :: app xs ys An expressive type system can also serve as a valuable maintenance tool , in that refactoring code can be made relatively painless by revealing which sites still have inconsistent typing. Abstraction \u00b6 Type systems can typically be used to enforce modularity in larger software projects that allows the programmer to enforce structure as well as how public APIs can be used, subject to visibility and accessibility . The API surface of a module then typically becomes the type of the module \u2013 a well-defined contract between the provider and its users. Then, the possibility of hiding-away implementation details facilitates abstraction , to allow dependency on well-defined interfaces instead of possibily constantly-changing implementation details. Documentation \u00b6 Types typically also serve to be useful when trying to read programs. Consider the following two snippets, written in JavaScript and TypeScript, respectively. function add ( a , b ) { return a + b ; } function add ( a : number , b : number ) : number { return a + b ; } In the JavaScript version, we have no guarantee that we can even add a and b together, needless to say what the results of the addition is. We might produce a string as the result of adding \"a\" + \"b\" , when we might expect a number as the result. We don't really know what it is doing \u2013 what argument it takes, and what return value it yields. In languages with stronger static type systems, we can often encode the possibility of missing return values, and possible failure with types too (versus possibility of forgetting null checks): For example, in Rust, there is a [ checked_add ] function defined for all primitive integer types which explicitly requires the user to account for the possibility of addition overflow. assert ! ( 1 u8 . checked_add ( u8 :: MAX ) == None ); These types are able to encode valuable information that we can use to supplement our documentation so that not only are certain conditions documented, but that they are also statically checked and verified by our compilers. In Haskell and other programming languages that can guarantee purity , when effects are needed they are typically encoded in the types too. -- We can guarantee purity here \u2013 no external state will be modified. add :: Int -> Int -> Int -- We can explicitly see that we are doing I/O as side effects in the type signature, add_with_logging :: Int -> Int -> IO Int Such types are also invaluable when it comes to guaranteeing public API stability and detecting breaking changes, especially when many libraries and projects today use semver for versioning and stability indication. Language Safety \u00b6 What consitutes \" safety \" within a programming language is subject to what the language itself defines as its guaranteed \" safety \". Typically, it constitutes as what you cannot do to harm yourself in a particular language. A language's provided safety guarantees are guarantees that the language will protect its own abstractions . An example of this may be trying to write to a sequential buffer. A \"memory-safe\" language such as Rust may provide safe APIs that prevents the programmer from writing out-of-bounds memory, or else the program explicitly crashes or panics to prevent illegal access. We say that Rust protects its abstraction of memory safety by explicitly enforcing bounds check for read and write access to such arrays/slices. let v = vec ! [ 0 , 2 , 4 , 6 ]; println ! ( \"{}\" , v [ 6 ]); // it will panic! // alternatively, use the `get` or `get_mut` methods to explicitly handle potential failure: match v . get ( 6 ) { Ok ( _ ) => { unreachable ! (); } None => { println ! ( \"index out-of-bounds\" ); } } Contrast this with C, where memory-safety is not guaranteed by the language and the responsibility of ensuring in-bounds memory access is delegated to the programmer \u2013 i.e. out-of-bounds memory access is undefined behaviour and behaviour is subject to the particular compiler, optimization level, memory model and underlying operating system (if there is an OS!). Ideally, the OS would terminate the process if it detects illegal memory access through paged virtual memory , but this is not guaranteed. On embedded systems where you don't have an OS , this probably results in corrupted memory \u2013 the hardware is totally free to summon demons! // WARNING: undefined behaviour \u2013 here be dragons! #include <stdio.h> int main () { int x [] = { 0 , 2 , 4 , 6 }; // this *could* segfault, could print garbage, could summon a dragon. printf ( \"x[6] = %d \\n \" , x [ 6 ]); } Other examples of safety rules include (non-exhaustive): Lexical scoping Guarantess on call stack Aliasing XOR mutation (e.g. Rust's ownership and borrowing rules encoded by & and &mut statically, or [ Borrow ] and [ BorrowMut ] dynamically, as well as other types and constructs) An important distinct to make is that \\[ \\text{language safety} \\neq \\text{static type safety} \\] Since language safety guarantees are typically enforced by both static checks (i.e. syntactical soundness checks ) and dynamic checks (i.e. semantic soundness checks ). An example of this is Rust's [ assert! ] checks, for which unsafe code relies on assert! to enforce run-time invariants that, if violated could lead to unsafety. \u2014 excerpt from [ std::assert documentation] And so, we establish that \\[ \\text{language safety} = \\text{syntactical soundness (static checks)} \\cup \\text{semantic soundness (dynamic checks)} \\] Even if these \"safe\" languages provide safe operations for which the language provides checks that guarantee the integrity of their abstractions, theese languages typically also provide facilities for bypassing these checks \u2013 either for performance reasons, or because there may be perfectly sound programs that are rejected by the type system which is purposefully conservative to preserve soundness. An example of this is the unsafe \"escape hatch\" provided by Rust \u2013 the programmer is allowed to bypass bounds checks, for example, but the responsibility of ensuring that the resulting program is semantically sound is up to the programmer. The vector Vec type provides an unsafe [ set_len ] method which forces the length of the vector to be a programmer-specified length. The language now does not do safety checks at run-time, but instead the responsibility of checking the required safety conditions are delegated to the programmer. That is, ensuring that The new_len must be less than or equal to capacity() . The elements at old_len..new_len must be initialized. \u2013 excerpt from [ set_len safety conditions] are now the responsibilities of the programmer. A [ debug_assert! ] is used in the implementation to do checks in debug builds, but such assertions are optimized out for release builds. In fact, an important reason as to why unsafe \"escape hatches\" are required even in \"safe\" languages is for Foreign Function Interfacing (FFI) . When a safe language calls over to another language (irrespective of whether the other language is safe or unsafe), they almost certainly will have different safety guarantees for which the safety guarantees may not be preserved over FFI boundaries . Cardelli (1996) provides another perspective of language safety: a language can be considered safe when run-time errors are \" trapped \", which causes the program to halt execution immediately (or via exceptions), while \" untrapped \" programs may allow execution to proceed \u2013 then a \"safe\" language in this perspective is one that prevents untrapped errors at run-time . Safety guarantees typically also have impacts on portability \u2013 when there are no safety guarantees for certain operations, and that the definition of the language leaves out what to do, then such behaviour is unspecified \u2013 aka \" Undefined Behaviour (UB) \". The compiler is typically free to do whatever it wishes when it encounters such UB, and typically UB behaves differently in different architectures and OS versions. In constrast, a well-typed program will produce identical results (up to allowed differences such as floating-point precision) under \"correct\" implementations. Efficiency \u00b6 Types provide additional information for the compilers w.r.t. to machine instruction selection as well as what optimizations are safe to perform. In safe languages that have a rich static type system, certain dynamic checks (e.g. bounds check) can be eliminated because there is sufficient type information that the compiler can use to statically prove that no out-of-bounds access can possibly occur. An example in Rust (compile this in Godbolt, try O3 versus turning off optimization, where panicking branches are not eliminated). Godbolt link: https://godbolt.org/z/EM6cTQ . pub fn main () { // by construction and type annotation, we know that `v` is an integer array consisting of exactly // 3 `u8` elements. let mut sum = 0 ; let v : [ u8 ; 3 ] = [ 1 , 2 , 3 ]; // by induction on this for-loop, we know that `i` can ever take a value in the inclusive range // [0, 2]. for i in 0 .. v . len () { // the compiler has sufficient information to prove that the `v[i]` access *cannot* be // out-of-bounds, so we don't need to generate `panic` branches for potential out-of-bounds // array index access. sum += i ; } println ! ( \"sum = {}\" , sum ); } Even compilers and interpreters for languages such as JavaScript which heavily relies on dynamic duck-typing and doesn't have a rich type system try hard to deduce and infer type information to try to perform optimizations. An example of this is the V8 JavaScript engine which does Just-In-Time (JIT) compilation of JavaScript code at run-time, which can accumlate run-time type information inductively. XKCD 1691: Optimization Further Applications \u00b6 Type systems can also help to enforce security . For example, there are research languages which attempt to encode security syntactically by annotating the flow of information . Type systems can also be used to define templates for serialization/deserialization automatically through code-gen, provided sufficient type information is available. A great example of this is provided by the [ serde ] library in Rust \u2013 the programmer can simply define a struct or enum , and have the library derive the relevent serialization/deserialization through macros. Type systems (albeit very powerful ones) can be used for automatic theorem proving . Examples of this include Coq or Agda .","title":"What Type Systems are Good For"},{"location":"introduction/type-systems-good-for/#what-are-type-systems-good-for","text":"","title":"What are Type Systems Good For?"},{"location":"introduction/type-systems-good-for/#error-detection","text":"Static type checking systems can detect and thus catch certain errors early, at compile-time. How much of such errors that can be caught by the type checking system depends on the type system's expressiveness . A more expressive type system typically allows the programmer to encode more information about data types and operations defined w.r.t. to those data types, which the type system can use these encoded information to check for certain errors. Languages which support dependent typing can express array bounds checks by encoding an array or vector's length in the types, so that bounds checking can be eliminated at run-time because the static type system can prove the absence of certain out-of-bounds access at compile-time. An example in Idris : -- Natural numbers: base case 0, or inductively define as successors of 0. -- e.g. 1 == S Z, 2 == S (S Z), and so on for all natural numbers. data Nat = Z | S Nat -- A vector is also inductively defined; it is either an empty list `Nil`, or elements appended -- to the empty list via `(::)`. Notice that the vector's length is encoded in its type, -- specifically with it's `Nat` type in `Vector k a`, which is parametric over both its length `k` -- and element type `a`. data Vector : Nat -> Type -> Type where Nil : Vector Z a ( :: ) : a -> Vector k a -> Vector ( S k ) a -- When two vectors are concatenated, the length of the resulting vector `r` is the sum of the -- lengths of the two input vectors `n` and `m` respectively. -- This invariant is encoded at the type level by the type-level expression `(n + m)`. concat : Vector n a -> Vector m a -> Vector ( n + m ) a concat Nil ys = ys concat ( x :: xs ) ys = x :: app xs ys An expressive type system can also serve as a valuable maintenance tool , in that refactoring code can be made relatively painless by revealing which sites still have inconsistent typing.","title":"Error Detection"},{"location":"introduction/type-systems-good-for/#abstraction","text":"Type systems can typically be used to enforce modularity in larger software projects that allows the programmer to enforce structure as well as how public APIs can be used, subject to visibility and accessibility . The API surface of a module then typically becomes the type of the module \u2013 a well-defined contract between the provider and its users. Then, the possibility of hiding-away implementation details facilitates abstraction , to allow dependency on well-defined interfaces instead of possibily constantly-changing implementation details.","title":"Abstraction"},{"location":"introduction/type-systems-good-for/#documentation","text":"Types typically also serve to be useful when trying to read programs. Consider the following two snippets, written in JavaScript and TypeScript, respectively. function add ( a , b ) { return a + b ; } function add ( a : number , b : number ) : number { return a + b ; } In the JavaScript version, we have no guarantee that we can even add a and b together, needless to say what the results of the addition is. We might produce a string as the result of adding \"a\" + \"b\" , when we might expect a number as the result. We don't really know what it is doing \u2013 what argument it takes, and what return value it yields. In languages with stronger static type systems, we can often encode the possibility of missing return values, and possible failure with types too (versus possibility of forgetting null checks): For example, in Rust, there is a [ checked_add ] function defined for all primitive integer types which explicitly requires the user to account for the possibility of addition overflow. assert ! ( 1 u8 . checked_add ( u8 :: MAX ) == None ); These types are able to encode valuable information that we can use to supplement our documentation so that not only are certain conditions documented, but that they are also statically checked and verified by our compilers. In Haskell and other programming languages that can guarantee purity , when effects are needed they are typically encoded in the types too. -- We can guarantee purity here \u2013 no external state will be modified. add :: Int -> Int -> Int -- We can explicitly see that we are doing I/O as side effects in the type signature, add_with_logging :: Int -> Int -> IO Int Such types are also invaluable when it comes to guaranteeing public API stability and detecting breaking changes, especially when many libraries and projects today use semver for versioning and stability indication.","title":"Documentation"},{"location":"introduction/type-systems-good-for/#language-safety","text":"What consitutes \" safety \" within a programming language is subject to what the language itself defines as its guaranteed \" safety \". Typically, it constitutes as what you cannot do to harm yourself in a particular language. A language's provided safety guarantees are guarantees that the language will protect its own abstractions . An example of this may be trying to write to a sequential buffer. A \"memory-safe\" language such as Rust may provide safe APIs that prevents the programmer from writing out-of-bounds memory, or else the program explicitly crashes or panics to prevent illegal access. We say that Rust protects its abstraction of memory safety by explicitly enforcing bounds check for read and write access to such arrays/slices. let v = vec ! [ 0 , 2 , 4 , 6 ]; println ! ( \"{}\" , v [ 6 ]); // it will panic! // alternatively, use the `get` or `get_mut` methods to explicitly handle potential failure: match v . get ( 6 ) { Ok ( _ ) => { unreachable ! (); } None => { println ! ( \"index out-of-bounds\" ); } } Contrast this with C, where memory-safety is not guaranteed by the language and the responsibility of ensuring in-bounds memory access is delegated to the programmer \u2013 i.e. out-of-bounds memory access is undefined behaviour and behaviour is subject to the particular compiler, optimization level, memory model and underlying operating system (if there is an OS!). Ideally, the OS would terminate the process if it detects illegal memory access through paged virtual memory , but this is not guaranteed. On embedded systems where you don't have an OS , this probably results in corrupted memory \u2013 the hardware is totally free to summon demons! // WARNING: undefined behaviour \u2013 here be dragons! #include <stdio.h> int main () { int x [] = { 0 , 2 , 4 , 6 }; // this *could* segfault, could print garbage, could summon a dragon. printf ( \"x[6] = %d \\n \" , x [ 6 ]); } Other examples of safety rules include (non-exhaustive): Lexical scoping Guarantess on call stack Aliasing XOR mutation (e.g. Rust's ownership and borrowing rules encoded by & and &mut statically, or [ Borrow ] and [ BorrowMut ] dynamically, as well as other types and constructs) An important distinct to make is that \\[ \\text{language safety} \\neq \\text{static type safety} \\] Since language safety guarantees are typically enforced by both static checks (i.e. syntactical soundness checks ) and dynamic checks (i.e. semantic soundness checks ). An example of this is Rust's [ assert! ] checks, for which unsafe code relies on assert! to enforce run-time invariants that, if violated could lead to unsafety. \u2014 excerpt from [ std::assert documentation] And so, we establish that \\[ \\text{language safety} = \\text{syntactical soundness (static checks)} \\cup \\text{semantic soundness (dynamic checks)} \\] Even if these \"safe\" languages provide safe operations for which the language provides checks that guarantee the integrity of their abstractions, theese languages typically also provide facilities for bypassing these checks \u2013 either for performance reasons, or because there may be perfectly sound programs that are rejected by the type system which is purposefully conservative to preserve soundness. An example of this is the unsafe \"escape hatch\" provided by Rust \u2013 the programmer is allowed to bypass bounds checks, for example, but the responsibility of ensuring that the resulting program is semantically sound is up to the programmer. The vector Vec type provides an unsafe [ set_len ] method which forces the length of the vector to be a programmer-specified length. The language now does not do safety checks at run-time, but instead the responsibility of checking the required safety conditions are delegated to the programmer. That is, ensuring that The new_len must be less than or equal to capacity() . The elements at old_len..new_len must be initialized. \u2013 excerpt from [ set_len safety conditions] are now the responsibilities of the programmer. A [ debug_assert! ] is used in the implementation to do checks in debug builds, but such assertions are optimized out for release builds. In fact, an important reason as to why unsafe \"escape hatches\" are required even in \"safe\" languages is for Foreign Function Interfacing (FFI) . When a safe language calls over to another language (irrespective of whether the other language is safe or unsafe), they almost certainly will have different safety guarantees for which the safety guarantees may not be preserved over FFI boundaries . Cardelli (1996) provides another perspective of language safety: a language can be considered safe when run-time errors are \" trapped \", which causes the program to halt execution immediately (or via exceptions), while \" untrapped \" programs may allow execution to proceed \u2013 then a \"safe\" language in this perspective is one that prevents untrapped errors at run-time . Safety guarantees typically also have impacts on portability \u2013 when there are no safety guarantees for certain operations, and that the definition of the language leaves out what to do, then such behaviour is unspecified \u2013 aka \" Undefined Behaviour (UB) \". The compiler is typically free to do whatever it wishes when it encounters such UB, and typically UB behaves differently in different architectures and OS versions. In constrast, a well-typed program will produce identical results (up to allowed differences such as floating-point precision) under \"correct\" implementations.","title":"Language Safety"},{"location":"introduction/type-systems-good-for/#efficiency","text":"Types provide additional information for the compilers w.r.t. to machine instruction selection as well as what optimizations are safe to perform. In safe languages that have a rich static type system, certain dynamic checks (e.g. bounds check) can be eliminated because there is sufficient type information that the compiler can use to statically prove that no out-of-bounds access can possibly occur. An example in Rust (compile this in Godbolt, try O3 versus turning off optimization, where panicking branches are not eliminated). Godbolt link: https://godbolt.org/z/EM6cTQ . pub fn main () { // by construction and type annotation, we know that `v` is an integer array consisting of exactly // 3 `u8` elements. let mut sum = 0 ; let v : [ u8 ; 3 ] = [ 1 , 2 , 3 ]; // by induction on this for-loop, we know that `i` can ever take a value in the inclusive range // [0, 2]. for i in 0 .. v . len () { // the compiler has sufficient information to prove that the `v[i]` access *cannot* be // out-of-bounds, so we don't need to generate `panic` branches for potential out-of-bounds // array index access. sum += i ; } println ! ( \"sum = {}\" , sum ); } Even compilers and interpreters for languages such as JavaScript which heavily relies on dynamic duck-typing and doesn't have a rich type system try hard to deduce and infer type information to try to perform optimizations. An example of this is the V8 JavaScript engine which does Just-In-Time (JIT) compilation of JavaScript code at run-time, which can accumlate run-time type information inductively. XKCD 1691: Optimization","title":"Efficiency"},{"location":"introduction/type-systems-good-for/#further-applications","text":"Type systems can also help to enforce security . For example, there are research languages which attempt to encode security syntactically by annotating the flow of information . Type systems can also be used to define templates for serialization/deserialization automatically through code-gen, provided sufficient type information is available. A great example of this is provided by the [ serde ] library in Rust \u2013 the programmer can simply define a struct or enum , and have the library derive the relevent serialization/deserialization through macros. Type systems (albeit very powerful ones) can be used for automatic theorem proving . Examples of this include Coq or Agda .","title":"Further Applications"},{"location":"introduction/type-systems-language-design/","text":"Type Systems and Language Design \u00b6 It's very difficult to try to \"retrofit\" a type system for a language that did not have type checking designed in mind. Even if a language did, great care has to be taken to ensure that no unsound public APIs are introduced which might not be able to be removed for a long time due to a language's stability/backwards-compatibility guarantees. Even for Rust, there still is not a formally specified denotational semantics for its type system (there's a subset of Rust's semantics, with work being spearheaded by the RustBelt Project , such as the \"Stacked Borrows\" paper by Ralf Jung et al. ). Work is also being done on the front of sepcifying operational semantics for Rust's type system in miri . Typically, the concrete syntax ( always subject to extensive bike-shedding ) for statically-typed languages is going to be more involved than untyped or dynamically-typed languages in order for the type-checker to have sufficient type information to work with at compile-time. When the type system is properly taken into account, designing a clean, comprehensible and intuitive syntax is easier.","title":"Type Systems and Language Design"},{"location":"introduction/type-systems-language-design/#type-systems-and-language-design","text":"It's very difficult to try to \"retrofit\" a type system for a language that did not have type checking designed in mind. Even if a language did, great care has to be taken to ensure that no unsound public APIs are introduced which might not be able to be removed for a long time due to a language's stability/backwards-compatibility guarantees. Even for Rust, there still is not a formally specified denotational semantics for its type system (there's a subset of Rust's semantics, with work being spearheaded by the RustBelt Project , such as the \"Stacked Borrows\" paper by Ralf Jung et al. ). Work is also being done on the front of sepcifying operational semantics for Rust's type system in miri . Typically, the concrete syntax ( always subject to extensive bike-shedding ) for statically-typed languages is going to be more involved than untyped or dynamically-typed languages in order for the type-checker to have sufficient type information to work with at compile-time. When the type system is properly taken into account, designing a clean, comprehensible and intuitive syntax is easier.","title":"Type Systems and Language Design"},{"location":"introduction/types-in-cs/","text":"Types in Computer Science \u00b6 Software engineering employs a wide range of formal methods to check that some system behaviours correctly when compared against a predefined set of specifications . [ system ] | v < formal method(s) > <- [ specification ] | v { is system correct w.r.t. specification? } | | v v { yes } { no } But typically, there exists a trade-off between generality (how expressive the verfication system is) and usability (how easy it is for the programmer to use the system), among other aspects such as time and space complexity. [Spectrum of Generality and Expressiveness of Formal Methods] More general/powerful Less general/powerful More cumbersome More light-weight Higher time/space complexity Less complex Requires more markup Less markup required <----------------------------------------------------------------------> Hoare logic Model checkers Algebraic verification languages Run-time monitoring Modal logics Type systems Denotational semantics Type systems is one of such lightweight formal method for checking the system against a specification automatically. Possible Definition of a \"Type System\" \u00b6 A type system is a tractable syntactic method for proving the absence of certain program behaviors by classifying phrases according to the kinds of values they compute. Focuses on programs . Classification of syntactical phrases (terms); computing a static approximation to the run-time behaviour of terms within a program. Since type systems are static (i.e. type information generated at compile-time, even for supposedly dynamically-checked or dynamically-typed languages where heap allocations are given type tags to do the type-checking at run-time; but this type tag information is still generated at compile-time). A Necessary Trade-off between Soundness and Completeness \u00b6 Static type systems need to be conservative in order to remain sound : Not all ill-behaved programs can be rejected by the type system. Not all well-behaved programs can be accepted by the type system. Again, this is yet another case of G\u00f6del's Incompleteness Theorem 1 where we must pick soundness (consistency) over completeness because we never want to mistakenly accept a \"badly-behaved\" program as a well-typed program. Our programming languages are sufficiently complex that the well-typedness of an arbitrary program is an undecidable problem . Any consistent formal system \\(F\\) within which a certain amount of elementary arithmetic can be carried out is incomplete; i.e., there are statements of the language of \\(F\\) which can neither be proved nor disproved in \\(F\\) . Due to the incompleteness, the type systems are also not able to catch every possible misbehaved program. The soundness w.r.t. bad behaviours only applies to the programs for which the typing rules of the type system can reject. The bad behaviours are typically eliminated by the type system through run-time type errors , but the safety (aka soundness ) of the type system must be judged w.r.t. to its own set of run-time errors . Voluntary Unsoundness: An Escape Hatch \u00b6 Interestingly, modern programming languages often do not necessarily have \"sound\" type systems in that precisely because some programs, despite behaving well at run-time, are not syntactically well-typed (i.e. the type systems are incomplete) \u2013 these type systems and programming languages provide \"unsafe\" mechanics to allow the programmer to write logic that the type system accepts as a true assumption but requires the programmer to manually enforce the required pre- and post-conditions and any required invariants. Examples: Java's sun.misc.Unsafe API. Rust's unsafe blocks and functions. C#'s unsafe keyword. The responsibility of checking the program's run-time behaviour when unsafe sections are used is then left to the programmer. fn main () { // safe bits... unsafe { // unsafe block \u2013 need to manually upload invariants } } // SAFETY: `f` requires additional invariants to be enforced by the caller unsafe fn f () {} \"With great power comes great responsibility\" \u2013 The Peter Parker Principle Some reseachers are exploring the possibility of elevating the soundness of a type system from just syntactical soundness up to semantic soundness \u2013 that is, a synatictically sound program is necessarily semantically sound , but a semantically sound program is not necessarily synatically sound (e.g. Rust unsafe sections). See the RustBelt project 2 , 3 . Working Together: Automatic Systems with Hints from the Programmer \u00b6 Type systems are automatic proof systems that typically also requires assistance from the programmer to suggest intent by providing type annotations . fn add ( x : u8 , y : u8 ) -> u8 { x + y } Here, the : TypeName and -> TypeName are Rust's syntactical type annotations. Rust enforces these type annotations for public APIs to preserve backwards compatibility of crates. Usually, a programming language keeps these annotations as light as possible without hindering type-checking so programmers do not need to type as much. Although, types also serve as excellent executable documentation that cannot become out of date. Efficiency of Type Systems \u00b6 It is insufficient for type systems to be sound \u2013 these type systems typically also need to be efficient so programmers don't have to suffer from long compile times, or in the case of dynamically type-checked languages, don't need to suffer from high run-time performance penalty. XKCD 303: Compiling Branches of the Study of Type Systems \u00b6 Within CS, there are two branches of the study of type systems: More practical approach w.r.t. to programming languages. More abstract (connections between various \"pure typed lambda-calculi\" and logics).","title":"Type Systems in Computer Science"},{"location":"introduction/types-in-cs/#types-in-computer-science","text":"Software engineering employs a wide range of formal methods to check that some system behaviours correctly when compared against a predefined set of specifications . [ system ] | v < formal method(s) > <- [ specification ] | v { is system correct w.r.t. specification? } | | v v { yes } { no } But typically, there exists a trade-off between generality (how expressive the verfication system is) and usability (how easy it is for the programmer to use the system), among other aspects such as time and space complexity. [Spectrum of Generality and Expressiveness of Formal Methods] More general/powerful Less general/powerful More cumbersome More light-weight Higher time/space complexity Less complex Requires more markup Less markup required <----------------------------------------------------------------------> Hoare logic Model checkers Algebraic verification languages Run-time monitoring Modal logics Type systems Denotational semantics Type systems is one of such lightweight formal method for checking the system against a specification automatically.","title":"Types in Computer Science"},{"location":"introduction/types-in-cs/#possible-definition-of-a-type-system","text":"A type system is a tractable syntactic method for proving the absence of certain program behaviors by classifying phrases according to the kinds of values they compute. Focuses on programs . Classification of syntactical phrases (terms); computing a static approximation to the run-time behaviour of terms within a program. Since type systems are static (i.e. type information generated at compile-time, even for supposedly dynamically-checked or dynamically-typed languages where heap allocations are given type tags to do the type-checking at run-time; but this type tag information is still generated at compile-time).","title":"Possible Definition of a \"Type System\""},{"location":"introduction/types-in-cs/#a-necessary-trade-off-between-soundness-and-completeness","text":"Static type systems need to be conservative in order to remain sound : Not all ill-behaved programs can be rejected by the type system. Not all well-behaved programs can be accepted by the type system. Again, this is yet another case of G\u00f6del's Incompleteness Theorem 1 where we must pick soundness (consistency) over completeness because we never want to mistakenly accept a \"badly-behaved\" program as a well-typed program. Our programming languages are sufficiently complex that the well-typedness of an arbitrary program is an undecidable problem . Any consistent formal system \\(F\\) within which a certain amount of elementary arithmetic can be carried out is incomplete; i.e., there are statements of the language of \\(F\\) which can neither be proved nor disproved in \\(F\\) . Due to the incompleteness, the type systems are also not able to catch every possible misbehaved program. The soundness w.r.t. bad behaviours only applies to the programs for which the typing rules of the type system can reject. The bad behaviours are typically eliminated by the type system through run-time type errors , but the safety (aka soundness ) of the type system must be judged w.r.t. to its own set of run-time errors .","title":"A Necessary Trade-off between Soundness and Completeness"},{"location":"introduction/types-in-cs/#voluntary-unsoundness-an-escape-hatch","text":"Interestingly, modern programming languages often do not necessarily have \"sound\" type systems in that precisely because some programs, despite behaving well at run-time, are not syntactically well-typed (i.e. the type systems are incomplete) \u2013 these type systems and programming languages provide \"unsafe\" mechanics to allow the programmer to write logic that the type system accepts as a true assumption but requires the programmer to manually enforce the required pre- and post-conditions and any required invariants. Examples: Java's sun.misc.Unsafe API. Rust's unsafe blocks and functions. C#'s unsafe keyword. The responsibility of checking the program's run-time behaviour when unsafe sections are used is then left to the programmer. fn main () { // safe bits... unsafe { // unsafe block \u2013 need to manually upload invariants } } // SAFETY: `f` requires additional invariants to be enforced by the caller unsafe fn f () {} \"With great power comes great responsibility\" \u2013 The Peter Parker Principle Some reseachers are exploring the possibility of elevating the soundness of a type system from just syntactical soundness up to semantic soundness \u2013 that is, a synatictically sound program is necessarily semantically sound , but a semantically sound program is not necessarily synatically sound (e.g. Rust unsafe sections). See the RustBelt project 2 , 3 .","title":"Voluntary Unsoundness: An Escape Hatch"},{"location":"introduction/types-in-cs/#working-together-automatic-systems-with-hints-from-the-programmer","text":"Type systems are automatic proof systems that typically also requires assistance from the programmer to suggest intent by providing type annotations . fn add ( x : u8 , y : u8 ) -> u8 { x + y } Here, the : TypeName and -> TypeName are Rust's syntactical type annotations. Rust enforces these type annotations for public APIs to preserve backwards compatibility of crates. Usually, a programming language keeps these annotations as light as possible without hindering type-checking so programmers do not need to type as much. Although, types also serve as excellent executable documentation that cannot become out of date.","title":"Working Together: Automatic Systems with Hints from the Programmer"},{"location":"introduction/types-in-cs/#efficiency-of-type-systems","text":"It is insufficient for type systems to be sound \u2013 these type systems typically also need to be efficient so programmers don't have to suffer from long compile times, or in the case of dynamically type-checked languages, don't need to suffer from high run-time performance penalty. XKCD 303: Compiling","title":"Efficiency of Type Systems"},{"location":"introduction/types-in-cs/#branches-of-the-study-of-type-systems","text":"Within CS, there are two branches of the study of type systems: More practical approach w.r.t. to programming languages. More abstract (connections between various \"pure typed lambda-calculi\" and logics).","title":"Branches of the Study of Type Systems"},{"location":"math-preliminaries/","text":"Math Preliminaries \u00b6 Some common notation and definitions. Can skip if the reader is already familiar with these concepts. XKCD 410: Math Paper","title":"Home"},{"location":"math-preliminaries/#math-preliminaries","text":"Some common notation and definitions. Can skip if the reader is already familiar with these concepts. XKCD 410: Math Paper","title":"Math Preliminaries"},{"location":"math-preliminaries/ordered-sets/exercises/","text":"Exercises \u00b6 Q1 \u00b6 Given relation \\(R\\) on set \\(S\\) ; define relation \\(R'\\) as \\[ R' \\triangleq R \\cup \\{ (s, s) \\mid s \\in S \\} \\] That is, \\(R'\\) contains all pairs in \\(R\\) plus all pairs of the form \\((s, s)\\) . Show \\(R'\\) is reflexive closure of \\(R\\) [TaPL]. Let \\(\\Delta_S \\triangleq \\{ (s, s) \\mid s \\in S \\}\\) be the diagonal relation on \\(S\\) , so that \\[ R' \\equiv R \\cup \\Delta_S \\] Diagonal Relation Let \\(S\\) be a set, then the diagonal relation on \\(S\\) , \\(\\Delta_S\\) , is defined as \\[ \\Delta_S \\triangleq \\{ (x, x) \\mid x \\in S \\} \\subseteq S \\times S \\] At most one relation on \\(S\\) can be the smallest reflexive superset of \\(R\\) , by Smallest Element is Unique theorem. Smallest Element is Unique Theorem \u00b6 Let \\((S, \\leq)\\) be an partially ordered set. If \\(S\\) has a smallest element, then there can only be one ( unique smallest element ). Proof \u00b6 Let \\(a, b \\in S\\) and both be smallest element of \\(S\\) and, since we have the partial order \\(\\leq\\) (let's call it relation \\(R\\) instead). Recall that partial order gives us antisymmetry : \\[ \\forall x, y \\in S \\colon (x, y) \\in R \\land (y, x) \\in R \\to x = y \\] \\[ \\begin{prooftree} \\AxiomC{$\\forall x, y \\in S \\colon (x, y) \\in R \\land (y, x) \\in R \\to x = y$} \\AxiomC{$\\forall y \\in S \\colon a \\leq y$} \\AxiomC{$\\forall y \\in S \\colon b \\leq y$} \\BinaryInfC{$a \\leq b \\land b \\leq a$} \\BinaryInfC{$a = b$} \\end{prooftree} \\] \\(R'\\) is a reflexive relation containing \\(R\\) , from \\[ \\begin{prooftree} \\AxiomC{$R' = R \\cup \\Delta_S$} \\UnaryInfC{$R \\subseteq R' \\land \\Delta_S \\subseteq R'$} \\end{prooftree} \\] We have \\(R'\\) is reflexive by Relation Contains Diagonal Relation theorem. Relation Contains Diagonal Relation iff Reflexive Theorem \u00b6 We have two equivalent definitions of a reflexive relation . Definition 1 \u00b6 \\(R\\) is reflexive iff: \\[ \\forall x \\in S \\colon (x, x) \\in R \\] Definition 2 \u00b6 \\(R\\) is reflexive iff it is a superset of the diagonal relation : \\[ \\Delta_S \\subseteq R \\] Proof \u00b6 Let \\(D_1\\) denote definition 1 and \\(D_2\\) denote definition 2. Definition 1 \\(\\implies\\) Definition 2 : Proof by contraposition. Required to prove that \\[ \\Delta_s \\not\\subseteq R \\to \\exists x \\in S \\colon (x, x) \\not\\in R \\] Suppose that \\(\\Delta_s \\not\\subseteq R\\) . \\[ \\begin{prooftree} \\AxiomC{$\\Delta_S \\not\\subseteq R$} \\RightLabel{(diagonal relation defn.)} \\UnaryInfC{$\\exists (x, x) \\in S \\times S \\colon (x, x) \\not\\in R$} \\UnaryInfC{$\\exists x \\in S \\colon (x, x) \\not\\in S$} \\RightLabel{(transposition)} \\UnaryInfC{$\\forall x \\in S \\colon (x, x) \\in R$} \\UnaryInfC{$\\Delta_S \\subseteq R$} \\end{prooftree} \\] Definition 2 \\(\\implies\\) Definition 1 : Let \\(R\\) be the relation that fulfills condition that \\(\\Delta_S \\subseteq R\\) . \\[ \\begin{prooftree} \\AxiomC{$\\Delta_S \\subseteq R$} \\RightLabel{(diagonal relation defn.)} \\UnaryInfC{$\\forall x \\in S \\colon (x, x) \\in \\Delta_S$} \\RightLabel{(subset)} \\UnaryInfC{$\\forall x \\in S \\colon (x, x) \\in R$} \\end{prooftree} \\] Which demonstrates that \\(R\\) is in fact reflexive by definition 1. And so the two definitions are equivalent . \\[ \\begin{prooftree} \\AxiomC{$D_1 \\to D_2$} \\AxiomC{$D_2 \\to D_1$} \\BinaryInfC{$D_1 \\leftrightarrow D_2$} \\end{prooftree} \\] This means that \\(R'\\) is a reflexive relation containing \\(R\\) . Every reflexive relation containing \\(R\\) must trivially also contain \\(\\Delta_S\\) , by the Relation Containing Diagonal Relation iff Reflexive theorem. \\(R'\\) is the smallest reflexive relation on \\(S\\) that contains \\(R\\) by the Union is Smallest Superset theorem. Union is Smallest Superset Theorem \u00b6 Let \\(S_1, S_2\\) be two sets. Then \\(S_1 \\cup S_2\\) is the smallest set containing both \\(S_1\\) and \\(S_2\\) . So for any set \\(T\\) : \\[ (S_1 \\subseteq T) \\land (S_2 \\subseteq T) \\leftrightarrow (S_1 \\cup S_2) \\subseteq T \\] Proof \u00b6 Prove by \\(A \\to B \\land B \\to A \\iff A \\leftrightarrow B\\) : Condition 1 : \\((S_1 \\subseteq T) \\land (S_2 \\subseteq T) \\to (S_1 \\cup S_2) \\subseteq T\\) : \\[ \\begin{prooftree} \\AxiomC{$(S_1 \\subseteq T) \\land (S_2 \\subseteq T)$} \\RightLabel{(union preserves subsets)} \\UnaryInfC{$(S_1 \\cup S_2) \\subseteq (T \\cup T)$} \\RightLabel{(union idempotence)} \\UnaryInfC{$(S_1 \\cup S_2) \\subseteq T$} \\end{prooftree} \\] Condition 2 : \\((S_1 \\cup S_2) \\subseteq T \\to (S_1 \\subseteq T) \\land (S_2 \\subseteq T)\\) : Assume \\((S_1 \\cup S_2) \\subseteq T\\) . \\[ \\begin{prooftree} \\AxiomC{$S_1 \\subseteq (S_1 \\cup S_2)$} \\UnaryInfC{$(S_1 \\cup S_2) \\subseteq T$} \\UnaryInfC{$S_1 \\subseteq T$} \\AxiomC{$S_2 \\subseteq (S_1 \\cup S_2)$} \\UnaryInfC{$(S_1 \\cup S_2) \\subseteq T$} \\UnaryInfC{$S_2 \\subseteq T$} \\BinaryInfC{$(S_1 \\cup S_2) \\subseteq T$} \\UnaryInfC{$(S_1 \\subseteq T) \\land (S_2 \\subseteq T)$} \\end{prooftree} \\] So, by equivalence , we have \\[ \\begin{prooftree} \\AxiomC{$(S_1 \\subseteq T) \\land (S_2 \\subseteq T) \\to (S_1 \\cup S_2) \\subseteq T$} \\AxiomC{$(S_1 \\cup S_2) \\subseteq T \\to (S_1 \\subseteq T) \\land (S_2 \\subseteq T)$} \\BinaryInfC{$(S_1 \\subseteq T) \\land (S_2 \\subseteq T) \\leftrightarrow (S_1 \\cup S_2) \\subseteq T$} \\end{prooftree} \\] Q2 \u00b6 Here is a more constructive definition of the transitive closure of a relation \\(R\\) . First, we define the following sequence of sets of pairs: \\[ \\begin{align} R_0 &= R \\\\ R_{i+1} &= R_i \\cup \\{ (s, u) \\mid \\exists t \\in S \\colon (s, t) \\in R_i \\land (t, u) \\in R_i \\} \\end{align} \\] That is, we construct each \\(R_{i+1}\\) by adding to \\(R_i\\) all the pairs that can be obtained by \"one step of transitivity\" from pairs already in \\(R_i\\) . Finally, define the relation \\(R^{+}\\) as the union of all \\(R_i\\) s. \\[ R^{+} = \\bigcup_{i} R_i \\] Show that this \\(R^{+}\\) is really the transitive closure of \\(R\\) . At most one relation on \\(S\\) can be the smallest transitive superset of \\(R\\) , by Smallest Element is Unique theorem. \\(R^{+}\\) is a transitive relation containing \\(R\\) , since by the principle of one-step mathematical induction. We demonstrate the proof for the proposition \\(P(i)\\) that every \\(R_i\\) (hence \\(R^{+}\\) ) contains \\(R\\) , for \\(i \\in \\mathbb{N}\\) . We first show that \\(R^{+}\\) contains \\(R\\) , which is trivial because by definition: \\[ R \\subseteq R_0 \\cup \\bigcup_{i > 0} R_i = R^{+} \\] Then we need to show that the relation is in fact transitive . We use complete induction to show this, for some \\(j \\in \\mathbb{N}\\) , we show that \\(\\forall i, j \\in \\mathbb{N} \\colon R_i \\subseteq R_j\\) . Assume \\(\\exists (a, b), (b, c) \\in R^{+}\\) . \\[ \\begin{prooftree} \\AxiomC{$\\exists (a, b), (b, c) \\in R^{+}$} \\UnaryInfC{$\\exists i \\in \\mathbb{N} \\colon (a, b) \\in R_i \\land \\exists j \\in \\mathbb{N} \\colon (b, c) \\in R_j$} \\UnaryInfC{$(a, b), (b, c) \\in R_{\\max{(i, j)}}$} \\UnaryInfC{$(a, c) \\in R_{\\max{(i, j) + 1}} \\subseteq R^{+}$} \\end{prooftree} \\] To show that \\(R^{+}\\) is the smallest transitive relation on \\(S\\) . Let \\(R'\\) be some transitive relation on \\(S\\) that contains \\(R\\) , we need to show that \\(\\forall n \\in \\mathbb{N} \\colon R_n \\subseteq R'\\) which means \\(R^{+} \\subseteq R'\\) . Trivially, \\(R_0 = R \\subseteq R'\\) . Assume that \\(R_n \\subseteq R'\\) and that \\((x, z) \\in R_{n+1}\\) : \\[ R_n \\subseteq R' \\land (x, z) \\in R_{n+1} \\to \\exists y \\colon (x, y) \\in R_n \\land (y, z) \\in R_n \\] Since \\(R_n \\subseteq R'\\) these pairs are too in \\(R'\\) , and since \\(R'\\) is transitive we have \\((x, z) \\in R'\\) , meaning \\(R^{+}\\) is in fact the smallest transitive relation on \\(S\\) .","title":"Exercises"},{"location":"math-preliminaries/ordered-sets/exercises/#exercises","text":"","title":"Exercises"},{"location":"math-preliminaries/ordered-sets/exercises/#q1","text":"Given relation \\(R\\) on set \\(S\\) ; define relation \\(R'\\) as \\[ R' \\triangleq R \\cup \\{ (s, s) \\mid s \\in S \\} \\] That is, \\(R'\\) contains all pairs in \\(R\\) plus all pairs of the form \\((s, s)\\) . Show \\(R'\\) is reflexive closure of \\(R\\) [TaPL]. Let \\(\\Delta_S \\triangleq \\{ (s, s) \\mid s \\in S \\}\\) be the diagonal relation on \\(S\\) , so that \\[ R' \\equiv R \\cup \\Delta_S \\] Diagonal Relation Let \\(S\\) be a set, then the diagonal relation on \\(S\\) , \\(\\Delta_S\\) , is defined as \\[ \\Delta_S \\triangleq \\{ (x, x) \\mid x \\in S \\} \\subseteq S \\times S \\] At most one relation on \\(S\\) can be the smallest reflexive superset of \\(R\\) , by Smallest Element is Unique theorem. Smallest Element is Unique","title":"Q1"},{"location":"math-preliminaries/ordered-sets/exercises/#theorem","text":"Let \\((S, \\leq)\\) be an partially ordered set. If \\(S\\) has a smallest element, then there can only be one ( unique smallest element ).","title":"Theorem"},{"location":"math-preliminaries/ordered-sets/exercises/#proof","text":"Let \\(a, b \\in S\\) and both be smallest element of \\(S\\) and, since we have the partial order \\(\\leq\\) (let's call it relation \\(R\\) instead). Recall that partial order gives us antisymmetry : \\[ \\forall x, y \\in S \\colon (x, y) \\in R \\land (y, x) \\in R \\to x = y \\] \\[ \\begin{prooftree} \\AxiomC{$\\forall x, y \\in S \\colon (x, y) \\in R \\land (y, x) \\in R \\to x = y$} \\AxiomC{$\\forall y \\in S \\colon a \\leq y$} \\AxiomC{$\\forall y \\in S \\colon b \\leq y$} \\BinaryInfC{$a \\leq b \\land b \\leq a$} \\BinaryInfC{$a = b$} \\end{prooftree} \\] \\(R'\\) is a reflexive relation containing \\(R\\) , from \\[ \\begin{prooftree} \\AxiomC{$R' = R \\cup \\Delta_S$} \\UnaryInfC{$R \\subseteq R' \\land \\Delta_S \\subseteq R'$} \\end{prooftree} \\] We have \\(R'\\) is reflexive by Relation Contains Diagonal Relation theorem. Relation Contains Diagonal Relation iff Reflexive","title":"Proof"},{"location":"math-preliminaries/ordered-sets/exercises/#theorem_1","text":"We have two equivalent definitions of a reflexive relation .","title":"Theorem"},{"location":"math-preliminaries/ordered-sets/exercises/#definition-1","text":"\\(R\\) is reflexive iff: \\[ \\forall x \\in S \\colon (x, x) \\in R \\]","title":"Definition 1"},{"location":"math-preliminaries/ordered-sets/exercises/#definition-2","text":"\\(R\\) is reflexive iff it is a superset of the diagonal relation : \\[ \\Delta_S \\subseteq R \\]","title":"Definition 2"},{"location":"math-preliminaries/ordered-sets/exercises/#proof_1","text":"Let \\(D_1\\) denote definition 1 and \\(D_2\\) denote definition 2. Definition 1 \\(\\implies\\) Definition 2 : Proof by contraposition. Required to prove that \\[ \\Delta_s \\not\\subseteq R \\to \\exists x \\in S \\colon (x, x) \\not\\in R \\] Suppose that \\(\\Delta_s \\not\\subseteq R\\) . \\[ \\begin{prooftree} \\AxiomC{$\\Delta_S \\not\\subseteq R$} \\RightLabel{(diagonal relation defn.)} \\UnaryInfC{$\\exists (x, x) \\in S \\times S \\colon (x, x) \\not\\in R$} \\UnaryInfC{$\\exists x \\in S \\colon (x, x) \\not\\in S$} \\RightLabel{(transposition)} \\UnaryInfC{$\\forall x \\in S \\colon (x, x) \\in R$} \\UnaryInfC{$\\Delta_S \\subseteq R$} \\end{prooftree} \\] Definition 2 \\(\\implies\\) Definition 1 : Let \\(R\\) be the relation that fulfills condition that \\(\\Delta_S \\subseteq R\\) . \\[ \\begin{prooftree} \\AxiomC{$\\Delta_S \\subseteq R$} \\RightLabel{(diagonal relation defn.)} \\UnaryInfC{$\\forall x \\in S \\colon (x, x) \\in \\Delta_S$} \\RightLabel{(subset)} \\UnaryInfC{$\\forall x \\in S \\colon (x, x) \\in R$} \\end{prooftree} \\] Which demonstrates that \\(R\\) is in fact reflexive by definition 1. And so the two definitions are equivalent . \\[ \\begin{prooftree} \\AxiomC{$D_1 \\to D_2$} \\AxiomC{$D_2 \\to D_1$} \\BinaryInfC{$D_1 \\leftrightarrow D_2$} \\end{prooftree} \\] This means that \\(R'\\) is a reflexive relation containing \\(R\\) . Every reflexive relation containing \\(R\\) must trivially also contain \\(\\Delta_S\\) , by the Relation Containing Diagonal Relation iff Reflexive theorem. \\(R'\\) is the smallest reflexive relation on \\(S\\) that contains \\(R\\) by the Union is Smallest Superset theorem. Union is Smallest Superset","title":"Proof"},{"location":"math-preliminaries/ordered-sets/exercises/#theorem_2","text":"Let \\(S_1, S_2\\) be two sets. Then \\(S_1 \\cup S_2\\) is the smallest set containing both \\(S_1\\) and \\(S_2\\) . So for any set \\(T\\) : \\[ (S_1 \\subseteq T) \\land (S_2 \\subseteq T) \\leftrightarrow (S_1 \\cup S_2) \\subseteq T \\]","title":"Theorem"},{"location":"math-preliminaries/ordered-sets/exercises/#proof_2","text":"Prove by \\(A \\to B \\land B \\to A \\iff A \\leftrightarrow B\\) : Condition 1 : \\((S_1 \\subseteq T) \\land (S_2 \\subseteq T) \\to (S_1 \\cup S_2) \\subseteq T\\) : \\[ \\begin{prooftree} \\AxiomC{$(S_1 \\subseteq T) \\land (S_2 \\subseteq T)$} \\RightLabel{(union preserves subsets)} \\UnaryInfC{$(S_1 \\cup S_2) \\subseteq (T \\cup T)$} \\RightLabel{(union idempotence)} \\UnaryInfC{$(S_1 \\cup S_2) \\subseteq T$} \\end{prooftree} \\] Condition 2 : \\((S_1 \\cup S_2) \\subseteq T \\to (S_1 \\subseteq T) \\land (S_2 \\subseteq T)\\) : Assume \\((S_1 \\cup S_2) \\subseteq T\\) . \\[ \\begin{prooftree} \\AxiomC{$S_1 \\subseteq (S_1 \\cup S_2)$} \\UnaryInfC{$(S_1 \\cup S_2) \\subseteq T$} \\UnaryInfC{$S_1 \\subseteq T$} \\AxiomC{$S_2 \\subseteq (S_1 \\cup S_2)$} \\UnaryInfC{$(S_1 \\cup S_2) \\subseteq T$} \\UnaryInfC{$S_2 \\subseteq T$} \\BinaryInfC{$(S_1 \\cup S_2) \\subseteq T$} \\UnaryInfC{$(S_1 \\subseteq T) \\land (S_2 \\subseteq T)$} \\end{prooftree} \\] So, by equivalence , we have \\[ \\begin{prooftree} \\AxiomC{$(S_1 \\subseteq T) \\land (S_2 \\subseteq T) \\to (S_1 \\cup S_2) \\subseteq T$} \\AxiomC{$(S_1 \\cup S_2) \\subseteq T \\to (S_1 \\subseteq T) \\land (S_2 \\subseteq T)$} \\BinaryInfC{$(S_1 \\subseteq T) \\land (S_2 \\subseteq T) \\leftrightarrow (S_1 \\cup S_2) \\subseteq T$} \\end{prooftree} \\]","title":"Proof"},{"location":"math-preliminaries/ordered-sets/exercises/#q2","text":"Here is a more constructive definition of the transitive closure of a relation \\(R\\) . First, we define the following sequence of sets of pairs: \\[ \\begin{align} R_0 &= R \\\\ R_{i+1} &= R_i \\cup \\{ (s, u) \\mid \\exists t \\in S \\colon (s, t) \\in R_i \\land (t, u) \\in R_i \\} \\end{align} \\] That is, we construct each \\(R_{i+1}\\) by adding to \\(R_i\\) all the pairs that can be obtained by \"one step of transitivity\" from pairs already in \\(R_i\\) . Finally, define the relation \\(R^{+}\\) as the union of all \\(R_i\\) s. \\[ R^{+} = \\bigcup_{i} R_i \\] Show that this \\(R^{+}\\) is really the transitive closure of \\(R\\) . At most one relation on \\(S\\) can be the smallest transitive superset of \\(R\\) , by Smallest Element is Unique theorem. \\(R^{+}\\) is a transitive relation containing \\(R\\) , since by the principle of one-step mathematical induction. We demonstrate the proof for the proposition \\(P(i)\\) that every \\(R_i\\) (hence \\(R^{+}\\) ) contains \\(R\\) , for \\(i \\in \\mathbb{N}\\) . We first show that \\(R^{+}\\) contains \\(R\\) , which is trivial because by definition: \\[ R \\subseteq R_0 \\cup \\bigcup_{i > 0} R_i = R^{+} \\] Then we need to show that the relation is in fact transitive . We use complete induction to show this, for some \\(j \\in \\mathbb{N}\\) , we show that \\(\\forall i, j \\in \\mathbb{N} \\colon R_i \\subseteq R_j\\) . Assume \\(\\exists (a, b), (b, c) \\in R^{+}\\) . \\[ \\begin{prooftree} \\AxiomC{$\\exists (a, b), (b, c) \\in R^{+}$} \\UnaryInfC{$\\exists i \\in \\mathbb{N} \\colon (a, b) \\in R_i \\land \\exists j \\in \\mathbb{N} \\colon (b, c) \\in R_j$} \\UnaryInfC{$(a, b), (b, c) \\in R_{\\max{(i, j)}}$} \\UnaryInfC{$(a, c) \\in R_{\\max{(i, j) + 1}} \\subseteq R^{+}$} \\end{prooftree} \\] To show that \\(R^{+}\\) is the smallest transitive relation on \\(S\\) . Let \\(R'\\) be some transitive relation on \\(S\\) that contains \\(R\\) , we need to show that \\(\\forall n \\in \\mathbb{N} \\colon R_n \\subseteq R'\\) which means \\(R^{+} \\subseteq R'\\) . Trivially, \\(R_0 = R \\subseteq R'\\) . Assume that \\(R_n \\subseteq R'\\) and that \\((x, z) \\in R_{n+1}\\) : \\[ R_n \\subseteq R' \\land (x, z) \\in R_{n+1} \\to \\exists y \\colon (x, y) \\in R_n \\land (y, z) \\in R_n \\] Since \\(R_n \\subseteq R'\\) these pairs are too in \\(R'\\) , and since \\(R'\\) is transitive we have \\((x, z) \\in R'\\) , meaning \\(R^{+}\\) is in fact the smallest transitive relation on \\(S\\) .","title":"Q2"},{"location":"math-preliminaries/ordered-sets/notes/","text":"Ordered Sets \u00b6 Reflexive, symmetric, transitive and antisymmetric binary relations \u00b6 A binary relation \\(R\\) on set \\(S\\) is reflexive iff \\[ \\forall s \\in S \\colon (s, s) \\in R \\] \\(R\\) is symmetric iff \\[ \\forall s, t \\in S \\colon (s, t) \\in R \\to (t, s) \\in R \\] \\(R\\) is transitive iff \\[ \\forall s, t, u \\in S \\colon (s, t) \\in R \\land (t, u)\\in R \\to (s, u) \\in R \\] \\(R\\) is antisymmetric iff \\[ \\forall s, t \\in S \\colon (s, t) \\in R \\land (t, s) \\in R \\to s = t \\] Preorder, partial order and total order \u00b6 A reflexive and transitive relation \\(R\\) on set \\(S\\) is a preorder on \\(S\\) . \\[ \\text{preorder} \\triangleq \\text{reflexive} \\land \\text{transitive} \\] Typically written using symbols such as \\(\\leq\\) or \\(\\sqsubseteq\\) . \\[ s < t \\leftrightarrow s \\leq t \\land s \\ne t \\] A preorder that is also antisymmetric is a partial order on \\(S\\) . \\[ \\text{partial order} \\triangleq \\text{reflexive} \\land \\text{transitive} \\land \\text{antisymmetric} \\] A partial order \\(\\leq\\) is a total order iff \\[ \\forall s, t \\in S \\colon s \\leq t \\lor t \\leq s \\] Join and meet \u00b6 Given partial order \\(\\leq\\) on set \\(S\\) , and elements \\(s, t \\in S\\) : An element \\(j \\in S\\) is a join (aka least upper bound ) of \\(s\\) and \\(t\\) iff \\(s \\leq j \\land t \\leq j\\) ; and \\(\\forall k \\in S \\colon s \\leq k \\land t \\leq k \\to j \\leq k\\) . An element \\(m \\in S\\) is a meet (aka greatest lower bound ) of \\(s\\) and \\(t\\) iff \\(m \\leq s \\land m \\leq t\\) ; and \\(\\forall n \\in S \\colon n \\leq s \\land n \\leq t \\to n \\leq m\\) . Equivalence \u00b6 A reflexive , transitive and symmetric relation on set \\(S\\) is an equivalence on \\(S\\) . \\[ \\text{equivalence} \\triangleq \\text{reflextive} \\land \\text{transitive} \\land \\text{symmetric} \\] Reflexive and transitive closure \u00b6 Let \\(R\\) be a binary relation on set \\(S\\) . The reflexive closure of \\(R\\) is the smallest reflexive relation \\(R'\\) that contains \\(R\\) . \"Smallest\" means if \\(R''\\) is another reflexive relation that contains all pairs in \\(R\\) , then \\(R' \\subseteq R''\\) . The transitive closure of \\(R\\) is the smallest transitive relation \\(R'\\) that contains \\(R\\) . Often written \\(R^+\\) . The reflexive and transtive closure of \\(R\\) is the smallest reflexive and transitive relation that contains \\(R\\) . Often written \\(R^\\ast\\) .","title":"Notes"},{"location":"math-preliminaries/ordered-sets/notes/#ordered-sets","text":"","title":"Ordered Sets"},{"location":"math-preliminaries/ordered-sets/notes/#reflexive-symmetric-transitive-and-antisymmetric-binary-relations","text":"A binary relation \\(R\\) on set \\(S\\) is reflexive iff \\[ \\forall s \\in S \\colon (s, s) \\in R \\] \\(R\\) is symmetric iff \\[ \\forall s, t \\in S \\colon (s, t) \\in R \\to (t, s) \\in R \\] \\(R\\) is transitive iff \\[ \\forall s, t, u \\in S \\colon (s, t) \\in R \\land (t, u)\\in R \\to (s, u) \\in R \\] \\(R\\) is antisymmetric iff \\[ \\forall s, t \\in S \\colon (s, t) \\in R \\land (t, s) \\in R \\to s = t \\]","title":"Reflexive, symmetric, transitive and antisymmetric binary relations"},{"location":"math-preliminaries/ordered-sets/notes/#preorder-partial-order-and-total-order","text":"A reflexive and transitive relation \\(R\\) on set \\(S\\) is a preorder on \\(S\\) . \\[ \\text{preorder} \\triangleq \\text{reflexive} \\land \\text{transitive} \\] Typically written using symbols such as \\(\\leq\\) or \\(\\sqsubseteq\\) . \\[ s < t \\leftrightarrow s \\leq t \\land s \\ne t \\] A preorder that is also antisymmetric is a partial order on \\(S\\) . \\[ \\text{partial order} \\triangleq \\text{reflexive} \\land \\text{transitive} \\land \\text{antisymmetric} \\] A partial order \\(\\leq\\) is a total order iff \\[ \\forall s, t \\in S \\colon s \\leq t \\lor t \\leq s \\]","title":"Preorder, partial order and total order"},{"location":"math-preliminaries/ordered-sets/notes/#join-and-meet","text":"Given partial order \\(\\leq\\) on set \\(S\\) , and elements \\(s, t \\in S\\) : An element \\(j \\in S\\) is a join (aka least upper bound ) of \\(s\\) and \\(t\\) iff \\(s \\leq j \\land t \\leq j\\) ; and \\(\\forall k \\in S \\colon s \\leq k \\land t \\leq k \\to j \\leq k\\) . An element \\(m \\in S\\) is a meet (aka greatest lower bound ) of \\(s\\) and \\(t\\) iff \\(m \\leq s \\land m \\leq t\\) ; and \\(\\forall n \\in S \\colon n \\leq s \\land n \\leq t \\to n \\leq m\\) .","title":"Join and meet"},{"location":"math-preliminaries/ordered-sets/notes/#equivalence","text":"A reflexive , transitive and symmetric relation on set \\(S\\) is an equivalence on \\(S\\) . \\[ \\text{equivalence} \\triangleq \\text{reflextive} \\land \\text{transitive} \\land \\text{symmetric} \\]","title":"Equivalence"},{"location":"math-preliminaries/ordered-sets/notes/#reflexive-and-transitive-closure","text":"Let \\(R\\) be a binary relation on set \\(S\\) . The reflexive closure of \\(R\\) is the smallest reflexive relation \\(R'\\) that contains \\(R\\) . \"Smallest\" means if \\(R''\\) is another reflexive relation that contains all pairs in \\(R\\) , then \\(R' \\subseteq R''\\) . The transitive closure of \\(R\\) is the smallest transitive relation \\(R'\\) that contains \\(R\\) . Often written \\(R^+\\) . The reflexive and transtive closure of \\(R\\) is the smallest reflexive and transitive relation that contains \\(R\\) . Often written \\(R^\\ast\\) .","title":"Reflexive and transitive closure"},{"location":"math-preliminaries/sets-functions-relations/","text":"Sets, Relations and Functions \u00b6 Sets \u00b6 Curly braces to list the elements a set explicitly: \\(\\{\\dots\\}\\) Set comprehension by construction: \\(\\{ x \\in S \\mid \\dots \\}\\) Empty set: \\(\\varnothing\\) Set difference of \\(S\\) and \\(T\\) : \\(S \\setminus T\\) Cardinality of set \\(S\\) : \\(\\lvert S \\rvert\\) Powerset of set \\(S\\) : \\(\\mathcal{P}(S)\\) Natural numbers \u00b6 Set of natural numbers denoted by \\(\\mathbb{N} = \\{ 0, 1, \\dots \\}\\) A set is countable iff its elements can form an injective mapping into the elements of the natural numbers. A set is countably infinite iff its elements can form a bijection to the elements of the natural numbers. n-place relation \u00b6 An \\(n\\) -place relation on the collection of sets \\(S_1, \\dots, S_n\\) is a set of tuples \\(R \\subseteq S_1 \\times \\cdots \\times S_n\\) of elements from each of the \\(S_i\\) sets. The elements \\(s_1 \\in S_1\\) to \\(s_n \\in S_n\\) are related by \\(R\\) \\(\\Longleftrightarrow (s_1, \\dots, s_n) \\in R\\) A one-place relation on a set \\(S\\) is a predicate on \\(S\\) . \\(P\\) is true for an element \\(s \\in S \\Longleftrightarrow s \\in P\\) . Often written as \\(P(s)\\) where \\(P \\colon S \\to \\{ \\top, \\bot \\}\\) . A two-place relation \\(R\\) on a two sets \\(S\\) and \\(T\\) is a binary relation . Often written \\(s\\ R\\ t\\) instead of \\((s, t) \\in R\\) for elements \\(s \\in S\\) and \\(t \\in T\\) . If \\(S = U \\land T = U\\) , \\(R\\) is a binary relation on \\(U\\) . Domain and range \u00b6 The domain \\(dom(R)\\) of relation \\(R\\) on sets \\(S\\) and \\(T\\) is \\[ \\{ s \\in S \\mid \\exists t \\in T \\colon (s, t) \\in R \\} \\] The range \\(range(R)\\) of relation \\(R\\) on sets \\(S\\) and \\(T\\) is \\[ \\{ t \\in T \\mid \\exists s \\in S \\colon (s, t) \\in R \\} \\] Partial function and total function \u00b6 The relation \\(R\\) on sets \\(S\\) and \\(T\\) is a partial function from \\(S\\) to \\(T\\) iff \\[ \\forall s \\in S, t_1, t_2 \\in T \\colon (s, t_1) \\in R \\land (s, t_2) \\in R \\rightarrow t_1 = t_2 \\] Iff \\(dom(R) = S\\) in addition, \\(R\\) is a total function (or simply function) from \\(S\\) to \\(T\\) . Defined and undefined \u00b6 For the partial function \\(R\\) from \\(S\\) to \\(T\\) , \\(R\\) is defined on argument \\(s \\in S\\) iff \\(s \\in dom(R)\\) , and undefined otherwise. \\(f(x)\\uparrow\\) or \\(f(x) = \\uparrow\\) means \\(f\\) is undefined on input \\(x\\) ; and \\(f(x)\\downarrow\\) means \\(f\\) is defined on input \\(x\\) . It is important to distinguish between undefined versus failure . Some functions may fail for certain inputs, which is a legit and observable result, versus divergence . A function that can fail can be either partial (i.e. can also diverge) or total (either return a result or fail explicitly). It is written \\(f(x) = fail\\) when \\(f\\) returns a failure result on input \\(x\\) . A function from \\(S\\) to \\(T\\) that may fail is really a function from \\(S\\) to \\(T \\cup \\{ \\text{fail} \\}\\) , where \\(\\text{fail}\\) is NOT an element of \\(T\\) . 1 Perservance of predicate \u00b6 Given binary relation \\(R\\) on set \\(S\\) and predicate \\(P\\) on \\(S\\) , \\(P\\) is preserved by \\(R\\) iff \\[ \\forall s, s' \\in S \\colon (s, s') \\in R \\land P(s) \\to P(s') \\] There may be optimization tricks such as Option<NonZeroU32> that make use of the invariant of NonZeroU32 that it can never be 0 to use that special value to indicate the Option is None . This is sound precisely because the cardinality of Option<NonZeroU32> matches the cardinality of u32 . Here, 0u32 is NOT an element of NonZeroU32 ! \u21a9","title":"Sets, Functions and Relations"},{"location":"math-preliminaries/sets-functions-relations/#sets-relations-and-functions","text":"","title":"Sets, Relations and Functions"},{"location":"math-preliminaries/sets-functions-relations/#sets","text":"Curly braces to list the elements a set explicitly: \\(\\{\\dots\\}\\) Set comprehension by construction: \\(\\{ x \\in S \\mid \\dots \\}\\) Empty set: \\(\\varnothing\\) Set difference of \\(S\\) and \\(T\\) : \\(S \\setminus T\\) Cardinality of set \\(S\\) : \\(\\lvert S \\rvert\\) Powerset of set \\(S\\) : \\(\\mathcal{P}(S)\\)","title":"Sets"},{"location":"math-preliminaries/sets-functions-relations/#natural-numbers","text":"Set of natural numbers denoted by \\(\\mathbb{N} = \\{ 0, 1, \\dots \\}\\) A set is countable iff its elements can form an injective mapping into the elements of the natural numbers. A set is countably infinite iff its elements can form a bijection to the elements of the natural numbers.","title":"Natural numbers"},{"location":"math-preliminaries/sets-functions-relations/#n-place-relation","text":"An \\(n\\) -place relation on the collection of sets \\(S_1, \\dots, S_n\\) is a set of tuples \\(R \\subseteq S_1 \\times \\cdots \\times S_n\\) of elements from each of the \\(S_i\\) sets. The elements \\(s_1 \\in S_1\\) to \\(s_n \\in S_n\\) are related by \\(R\\) \\(\\Longleftrightarrow (s_1, \\dots, s_n) \\in R\\) A one-place relation on a set \\(S\\) is a predicate on \\(S\\) . \\(P\\) is true for an element \\(s \\in S \\Longleftrightarrow s \\in P\\) . Often written as \\(P(s)\\) where \\(P \\colon S \\to \\{ \\top, \\bot \\}\\) . A two-place relation \\(R\\) on a two sets \\(S\\) and \\(T\\) is a binary relation . Often written \\(s\\ R\\ t\\) instead of \\((s, t) \\in R\\) for elements \\(s \\in S\\) and \\(t \\in T\\) . If \\(S = U \\land T = U\\) , \\(R\\) is a binary relation on \\(U\\) .","title":"n-place relation"},{"location":"math-preliminaries/sets-functions-relations/#domain-and-range","text":"The domain \\(dom(R)\\) of relation \\(R\\) on sets \\(S\\) and \\(T\\) is \\[ \\{ s \\in S \\mid \\exists t \\in T \\colon (s, t) \\in R \\} \\] The range \\(range(R)\\) of relation \\(R\\) on sets \\(S\\) and \\(T\\) is \\[ \\{ t \\in T \\mid \\exists s \\in S \\colon (s, t) \\in R \\} \\]","title":"Domain and range"},{"location":"math-preliminaries/sets-functions-relations/#partial-function-and-total-function","text":"The relation \\(R\\) on sets \\(S\\) and \\(T\\) is a partial function from \\(S\\) to \\(T\\) iff \\[ \\forall s \\in S, t_1, t_2 \\in T \\colon (s, t_1) \\in R \\land (s, t_2) \\in R \\rightarrow t_1 = t_2 \\] Iff \\(dom(R) = S\\) in addition, \\(R\\) is a total function (or simply function) from \\(S\\) to \\(T\\) .","title":"Partial function and total function"},{"location":"math-preliminaries/sets-functions-relations/#defined-and-undefined","text":"For the partial function \\(R\\) from \\(S\\) to \\(T\\) , \\(R\\) is defined on argument \\(s \\in S\\) iff \\(s \\in dom(R)\\) , and undefined otherwise. \\(f(x)\\uparrow\\) or \\(f(x) = \\uparrow\\) means \\(f\\) is undefined on input \\(x\\) ; and \\(f(x)\\downarrow\\) means \\(f\\) is defined on input \\(x\\) . It is important to distinguish between undefined versus failure . Some functions may fail for certain inputs, which is a legit and observable result, versus divergence . A function that can fail can be either partial (i.e. can also diverge) or total (either return a result or fail explicitly). It is written \\(f(x) = fail\\) when \\(f\\) returns a failure result on input \\(x\\) . A function from \\(S\\) to \\(T\\) that may fail is really a function from \\(S\\) to \\(T \\cup \\{ \\text{fail} \\}\\) , where \\(\\text{fail}\\) is NOT an element of \\(T\\) . 1","title":"Defined and undefined"},{"location":"math-preliminaries/sets-functions-relations/#perservance-of-predicate","text":"Given binary relation \\(R\\) on set \\(S\\) and predicate \\(P\\) on \\(S\\) , \\(P\\) is preserved by \\(R\\) iff \\[ \\forall s, s' \\in S \\colon (s, s') \\in R \\land P(s) \\to P(s') \\] There may be optimization tricks such as Option<NonZeroU32> that make use of the invariant of NonZeroU32 that it can never be 0 to use that special value to indicate the Option is None . This is sound precisely because the cardinality of Option<NonZeroU32> matches the cardinality of u32 . Here, 0u32 is NOT an element of NonZeroU32 ! \u21a9","title":"Perservance of predicate"},{"location":"resources/rust/","text":"Resources for the Rust Programming Language and its Ecosystem \u00b6 Rust A language empowering everyone to build reliable and efficient software. \u2013 https://www.rust-lang.org/ Rust the Language \u00b6 Learning Resources \u00b6 \"The Book\": https://doc.rust-lang.org/book/ Rustlings: https://github.com/rust-lang/rustlings/ Rust by Example: https://doc.rust-lang.org/stable/rust-by-example/ Learn Rust With Entirely Too Many Linked Lists: https://rust-unofficial.github.io/too-many-lists/ Programming Rust: https://www.oreilly.com/library/view/programming-rust-2nd/9781492052586/ easy_rust : https://github.com/Dhghomon/easy_rust Documentation \u00b6 std documentation: https://doc.rust-lang.org/std/ Hosted documentation for ecosystem crates: https://docs.rs Source Code \u00b6 The rustc compiler: https://github.com/rust-lang/rust polonius : https://github.com/rust-lang/polonius miri : https://github.com/rust-lang/miri rustc_codegen_cranelift : https://github.com/bjorn3/rustc_codegen_cranelift Rust's Ecosystem \u00b6 Build Tools \u00b6 cargo : https://github.com/rust-lang/cargo Many cargo extensions exist too! Build cache with sccache : https://github.com/mozilla/sccache Crates (Libraries) and Registries \u00b6 crates.io lib.rs Useful Tools \u00b6 Rust Search Extension: https://github.com/huhu/rust-search-extension mdBook : https://github.com/rust-lang/mdBook Useful CLI Tools Written in Rust \u00b6 bandwhich bat bingrep bottom cargo-asm cargo-audit cargo-bloat cargo-cache cargo-check cargo-deps cargo-edit cargo-license cargo-modules cargo-outdated cargo-release cargo-strip cargo-tree cargo-udeps cargo-update cargo-web cargo-xtask chit choose clog crates diffr diskus dust dua evcxr exa flamegraph git-journal honggfuzz hyperfine loc mdbook mdbook-linkcheck oxipng peep pueue rg rip rust-analyzer sd sfz silicon sk sniffglue so tiny tock vivid wasm-bindgen wasm-pack xsv zee zoxide","title":"Rust"},{"location":"resources/rust/#resources-for-the-rust-programming-language-and-its-ecosystem","text":"Rust A language empowering everyone to build reliable and efficient software. \u2013 https://www.rust-lang.org/","title":"Resources for the Rust Programming Language and its Ecosystem"},{"location":"resources/rust/#rust-the-language","text":"","title":"Rust the Language"},{"location":"resources/rust/#learning-resources","text":"\"The Book\": https://doc.rust-lang.org/book/ Rustlings: https://github.com/rust-lang/rustlings/ Rust by Example: https://doc.rust-lang.org/stable/rust-by-example/ Learn Rust With Entirely Too Many Linked Lists: https://rust-unofficial.github.io/too-many-lists/ Programming Rust: https://www.oreilly.com/library/view/programming-rust-2nd/9781492052586/ easy_rust : https://github.com/Dhghomon/easy_rust","title":"Learning Resources"},{"location":"resources/rust/#documentation","text":"std documentation: https://doc.rust-lang.org/std/ Hosted documentation for ecosystem crates: https://docs.rs","title":"Documentation"},{"location":"resources/rust/#source-code","text":"The rustc compiler: https://github.com/rust-lang/rust polonius : https://github.com/rust-lang/polonius miri : https://github.com/rust-lang/miri rustc_codegen_cranelift : https://github.com/bjorn3/rustc_codegen_cranelift","title":"Source Code"},{"location":"resources/rust/#rusts-ecosystem","text":"","title":"Rust's Ecosystem"},{"location":"resources/rust/#build-tools","text":"cargo : https://github.com/rust-lang/cargo Many cargo extensions exist too! Build cache with sccache : https://github.com/mozilla/sccache","title":"Build Tools"},{"location":"resources/rust/#crates-libraries-and-registries","text":"crates.io lib.rs","title":"Crates (Libraries) and Registries"},{"location":"resources/rust/#useful-tools","text":"Rust Search Extension: https://github.com/huhu/rust-search-extension mdBook : https://github.com/rust-lang/mdBook","title":"Useful Tools"},{"location":"resources/rust/#useful-cli-tools-written-in-rust","text":"bandwhich bat bingrep bottom cargo-asm cargo-audit cargo-bloat cargo-cache cargo-check cargo-deps cargo-edit cargo-license cargo-modules cargo-outdated cargo-release cargo-strip cargo-tree cargo-udeps cargo-update cargo-web cargo-xtask chit choose clog crates diffr diskus dust dua evcxr exa flamegraph git-journal honggfuzz hyperfine loc mdbook mdbook-linkcheck oxipng peep pueue rg rip rust-analyzer sd sfz silicon sk sniffglue so tiny tock vivid wasm-bindgen wasm-pack xsv zee zoxide","title":"Useful CLI Tools Written in Rust"},{"location":"resources/type-systems-logics/","text":"Resources for Type Systems and Logics \u00b6 Here we list various resources related to type systems and logics. Resources can include blogs, papers, programs and projects, etc. Resources are not grouped by topic, because a resource can span multiple topics and thus resources are instead labelled by multiple knowledge tags. work in progress ProofWiki https://proofwiki.org/wiki/Main_Page What Type Soundness Theorem Do You Really Want to Prove? Makes important distinction between semantic type soundness versus syntactic type soundness. D. Dreyer, A. Timany, R. Krebbers, L. Birkedal, and R. Jung, \u201cWhat Type Soundness Theorem Do You Really Want to Prove?,\u201d SIGPLAN Blog , 17-Oct-2019. [Online]. Available: https://blog.sigplan.org/2019/10/17/what-type-soundness-theorem-do-you-really-want-to-prove/ . [Accessed: 10-Sep-2020].","title":"Type Systems and Logics"},{"location":"resources/type-systems-logics/#resources-for-type-systems-and-logics","text":"Here we list various resources related to type systems and logics. Resources can include blogs, papers, programs and projects, etc. Resources are not grouped by topic, because a resource can span multiple topics and thus resources are instead labelled by multiple knowledge tags. work in progress ProofWiki https://proofwiki.org/wiki/Main_Page What Type Soundness Theorem Do You Really Want to Prove? Makes important distinction between semantic type soundness versus syntactic type soundness. D. Dreyer, A. Timany, R. Krebbers, L. Birkedal, and R. Jung, \u201cWhat Type Soundness Theorem Do You Really Want to Prove?,\u201d SIGPLAN Blog , 17-Oct-2019. [Online]. Available: https://blog.sigplan.org/2019/10/17/what-type-soundness-theorem-do-you-really-want-to-prove/ . [Accessed: 10-Sep-2020].","title":"Resources for Type Systems and Logics"}]}